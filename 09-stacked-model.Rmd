# Stacked Model with Decision Tree (SM)

<!-- ## R Packages and Setup {-} -->

```{r load_libraries_09, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(caret)
library(C50)
```


## Introduction

After completing all assigned model builds, we will construct a two-stage stacked ensemble.

In the initial stage, we will employ solely the fundamental L1 models as base learners.

In the subsequent stage, we will integrate all complex L1 models like the advanced LR and fine-tuned RF into the base learner set. Our primary objective is to maximize the predictive precision of the base models provided by L1 models through a multi-level stacked ensemble approach.


## Load Data

```{r load_data_09}
# Load data
test <- read.csv('Train Test Set/test.csv')
lr <- read.csv('Predictions/prediction_lr_probability.csv')
lr_complex <- read.csv('Predictions/prediction_lr_complex_probability.csv')
knn <- read.csv('Predictions/prediction_knn_probability.csv')
knn_tuned <- read.csv('Predictions/prediction_knn_tuned_probability.csv')
ann <- read.csv('Predictions/prediction_ann_probability.csv')
ann_complex <- read.csv('Predictions/prediction_ann_complex_probability.csv')
dt <- read.csv('Predictions/prediction_dt.csv')
dt_cost <- read.csv('Predictions/prediction_dt_cost.csv')
svm_vanilla <- read.csv('Predictions/prediction_svm_vanilla.csv')
svm_rbf <- read.csv('Predictions/prediction_svm_rbf.csv')
rf <- read.csv('Predictions/prediction_rf.csv')
rf_tuned <-read.csv('Predictions/prediction_rf_tuned.csv')
xgb <- read.csv('Predictions/prediction_xgb_probability.csv')
xgb_tuned <- read.csv('Predictions/prediction_xgb_tuned.csv')


# Display statistics
summary(test)
summary(lr)
summary(lr_complex)
summary(knn)
summary(knn_tuned)
summary(ann)
summary(ann_complex)
summary(dt)
summary(dt_cost)
summary(svm_vanilla)
summary(svm_rbf)
summary(rf)
summary(rf_tuned)
summary(xgb)
summary(xgb_tuned)
```

```{r combine_data_09}
# Combine data
stacked <- data.frame(
  lr$x,
  lr_complex$x,
  knn$x,
  knn_tuned$x,
  ann$V1,
  ann_complex$V1,
  dt$x,
  dt_cost$x,
  svm_vanilla$V1,
  svm_rbf$V1,
  rf$x,
  rf_tuned$x,
  xgb$x,
  xgb_tuned$x,
  test$obesity_leveloverweight
  )

# Display statistics
str(stacked)
summary(stacked)
head(stacked)
```


## Split Data

```{r split_data_09}
# Set seed for reproducibility
set.seed(12345)

# Define training set ratio
train_ratio <- 0.70

# Randomly sample training set
train_rows <- sample(1:nrow(stacked), train_ratio * nrow(stacked))

#### Logistics regression and Decision Tree Split
# Split data into training and testing sets
train_sm <- stacked[train_rows, ]
test_sm <- stacked[-train_rows, ]
```

## Define Cost Matrix

```{r define_cost_matrix_09}
# Build cost matrix
cost_matrix <- matrix(c(0, 8, 1, 0), nrow = 2, byrow = TRUE,
                         dimnames = list(Actual = c('0', '1'),
                                         Predicted = c('0', '1')))
```


## Model SM

### Simple SM

```{r model_sm_09, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}

# Build a model
model_sm <- C5.0(
  as.factor(test.obesity_leveloverweight) ~ .,
  data = train_sm
  )
```

```{r model_sm_summary_09}
# Display summary
summary(model_sm)
plot(model_sm)
varImp(model_sm)
```

According to the plot, it is apparent that the second-level stacked model relies solely on random forest as its predictor for decision making.

### SM with Cost Matrix

```{r model_sm_cost_09, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
# Build a model
model_sm_cost <- C5.0(
  as.factor(test.obesity_leveloverweight) ~ .,
  data = train_sm,
  costs = cost_matrix
  )
```

```{r model_sm_cost_summary_09}
# Display summary
summary(model_sm_cost)
plot(model_sm_cost)
varImp(model_sm_cost)
```

Incorporating a cost matrix into our stacked model leads to observable differences. According to the plot, L1 DT model with cost matrix and RBF SVM have emerged as top predictors, while LR is now involved in decision making and ANN were implemented during the final stages.

## Evaluate SM

### Simple SM

```{r evalute_sm_09}
# Make a prediction
prediction_sm <- predict(model_sm, test_sm)
prediction_sm_prob <- predict(model_sm, test_sm, type = 'prob')

summary(prediction_sm)

# Perform confusion matrix
cm_sm <- confusionMatrix(
  as.factor(prediction_sm),
  as.factor(test_sm$test.obesity_leveloverweight),
  positive = '1'
  )
cm_sm
```

Based on the confusion matrix, we observe that the model achieves an accuracy of ``r round(cm_sm$overall[1], 4)``, sensitivity of ``r round(cm_sm$byClass[1], 4)``, and a Kappa coefficient of ``r round(cm_sm$overall[2], 4)``. We will assess these results towards the conclusion of the assignment when we have completed stacked models.

```{r plot_auc_sm_09}
# Plot AUC
pred <- ROCR::prediction(
  prediction_sm_prob[,2],
  test_sm$test.obesity_leveloverweight
  )
perf <- ROCR::performance(pred, measure = "tpr", x.measure = "fpr")
auc <- ROCR::performance(pred, measure="auc")

auc <- round(auc@y.values[[1]],3)
roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="GLM")

ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2, fill = "blue") +
    geom_line(aes(y=tpr), col = "blue") +
    geom_abline(intercept = 0, slope = 1, lty = "dashed") +
    labs(title = paste0("Simple Stacked Model ROC Curve w/ AUC=", auc)) +
    theme_bw()
```

### SM with Cost Matrix

```{r evalute_sm_cost_09}
# Make a prediction
prediction_sm_cost <- predict(model_sm_cost, test_sm)
summary(prediction_sm_cost)

# Perform confusion matrix
cm_sm_cost <- confusionMatrix(
  as.factor(prediction_sm_cost),
  as.factor(test_sm$test.obesity_leveloverweight),
  positive = '1'
  )
cm_sm_cost
```

Based on the confusion matrix, we observe that the model achieves an accuracy of ``r round(cm_sm_cost$overall[1], 4)``, sensitivity of ``r round(cm_sm_cost$byClass[1], 4)``, and a Kappa coefficient of ``r round(cm_sm_cost$overall[2], 4)``. We will assess these results towards the conclusion of the assignment when we have completed stacked models.


## Model Evaluations

With all models completed, we will now assess their performance and compile a comprehensive accuracy table that showcases essential metrics for each model. In subsequent stages, we will explore the business implications to determine the potential impact of our models.

```{r model_evaluation_09}
# Load data
file_paths <- c(
  'Confusion Matrix/cm_lr.csv',
  'Confusion Matrix/cm_lr_complex.csv',
  'Confusion Matrix/cm_knn.csv',
  'Confusion Matrix/cm_knn_tuned.csv',
  'Confusion Matrix/cm_ann.csv',
  'Confusion Matrix/cm_ann_complex.csv',
  'Confusion Matrix/cm_dt.csv',
  'Confusion Matrix/cm_dt_cost.csv',
  'Confusion Matrix/cm_svm_vanilla.csv',
  'Confusion Matrix/cm_svm_rbf.csv',
  'Confusion Matrix/cm_rf.csv',
  'Confusion Matrix/cm_rf_tuned.csv',
  'Confusion Matrix/cm_xgb.csv',
  'Confusion Matrix/cm_xgb_tuned.csv'
)

# Store confusion matrices
cms <- list()
for (i in seq_along(file_paths)) {
  cm <- read.csv(file_paths[i])
  cm_obj <- confusionMatrix(matrix(c(cm[1,1],
                                     cm[1,2],
                                     cm[2,1],
                                     cm[2,2]), 
                                   nrow = 2, byrow = TRUE), 
                            positive = 'B')
  var_name <- tools::file_path_sans_ext(basename(file_paths[i]))
  cms[[var_name]] <- cm_obj
}

# Extract accuracy
accuracy_table <- data.frame(
  Model = c(
    'LR', 'LR Complex',
    'KNN', 'KNN Tuned',
    'ANN', 'ANN Complex',
    'DT', 'DT with Cost',
    'SVM Vanilla', 'SVM RBF',
    'RF', 'RF Tuned',
    'XGB', 'XGB Tuned',
    'SM', 'SM with Cost'
    ),
  Accuracy = c(
    round(cms[['cm_lr']]$overall[1], digits = 4),
    round(cms[['cm_lr_complex']]$overall[1], digits = 4),
    round(cms[['cm_knn']]$overall[1], digits = 4),
    round(cms[['cm_knn_tuned']]$overall[1], digits = 4),
    round(cms[['cm_ann']]$overall[1], digits = 4),
    round(cms[['cm_ann_complex']]$overall[1], digits = 4),
    round(cms[['cm_dt']]$overall[1], digits = 4),
    round(cms[['cm_dt_cost']]$overall[1], digits = 4),
    round(cms[['cm_svm_vanilla']]$overall[1], digits = 4),
    round(cms[['cm_svm_rbf']]$overall[1], digits = 4),
    round(cms[['cm_rf']]$overall[1], digits = 4),
    round(cms[['cm_rf_tuned']]$overall[1], digits = 4),
    round(cms[['cm_xgb']]$overall[1], digits = 4),
    round(cms[['cm_xgb_tuned']]$overall[1], digits = 4),
    round(cm_sm$overall[1], digits = 4),
    round(cm_sm_cost$overall[1], digits = 4)
    ),
  Sensitivity = c(
    round(cms[['cm_lr']]$byClass[1], digits = 4),
    round(cms[['cm_lr_complex']]$byClass[1], digits = 4),
    round(cms[['cm_knn']]$byClass[1], digits = 4),
    round(cms[['cm_knn_tuned']]$byClass[1], digits = 4),
    round(cms[['cm_ann']]$byClass[1], digits = 4),
    round(cms[['cm_ann_complex']]$byClass[1], digits = 4),
    round(cms[['cm_dt']]$byClass[1], digits = 4),
    round(cms[['cm_dt_cost']]$byClass[1], digits = 4),
    round(cms[['cm_svm_vanilla']]$byClass[1], digits = 4),
    round(cms[['cm_svm_rbf']]$byClass[1], digits = 4),
    round(cms[['cm_rf']]$byClass[1], digits = 4),
    round(cms[['cm_rf_tuned']]$byClass[1], digits = 4),
    round(cms[['cm_xgb']]$byClass[1], digits = 4),
    round(cms[['cm_xgb_tuned']]$byClass[1], digits = 4),
    round(cm_sm$byClass[1], digits = 4),
    round(cm_sm_cost$byClass[1], digits = 4)
    ),
  Kappa = c(
    round(cms[['cm_lr']]$overall[2], digits = 4),
    round(cms[['cm_lr_complex']]$overall[2], digits = 4),
    round(cms[['cm_knn']]$overall[2], digits = 4),
    round(cms[['cm_knn_tuned']]$overall[2], digits = 4),
    round(cms[['cm_ann']]$overall[2], digits = 4),
    round(cms[['cm_ann_complex']]$overall[2], digits = 4),
    round(cms[['cm_dt']]$overall[2], digits = 4),
    round(cms[['cm_dt_cost']]$overall[2], digits = 4),
    round(cms[['cm_svm_vanilla']]$overall[2], digits = 4),
    round(cms[['cm_svm_rbf']]$overall[2], digits = 4),
    round(cms[['cm_rf']]$overall[2], digits = 4),
    round(cms[['cm_rf_tuned']]$overall[2], digits = 4),
    round(cms[['cm_xgb']]$overall[2], digits = 4),
    round(cms[['cm_xgb_tuned']]$overall[2], digits = 4),
    round(cm_sm$overall[2], digits = 4),
    round(cm_sm_cost$overall[2], digits = 4)
    )
  )
accuracy_table <- accuracy_table[order(
  accuracy_table$Accuracy,
  decreasing = TRUE), ]
rownames(accuracy_table) <- NULL

# Make table into kable
kable_output <- kable(accuracy_table, caption = "Accuracy Table by Model", format = "html") |>
  kable_styling(bootstrap_options = c("striped", "hover")) |>
  footnote(general = "All metrics are calculated using the test set.",
           general_title = "Please Note:",
           footnote_as_chunk = TRUE)
kable_output
```




<!-- ```{r profit_table} -->
<!-- # # List models -->
<!-- # model_names <- c('cm_lr', 'cm_lr_complex', 'cm_knn', 'cm_knn_tuned', 'cm_ann', 'cm_ann_complex', 'cm_dt', 'cm_dt_cost', 'cm_svm_vanilla', 'cm_svm_rbf', 'cm_rf', 'cm_rf_tuned') -->
<!-- #  -->
<!-- # # Input known parameters -->
<!-- # cost_per_call <- 1 -->
<!-- # revenue_per_successful_call <- 6 -->
<!-- #  -->
<!-- # # Initialize profits empty list -->
<!-- # profits <- list() -->
<!-- #  -->
<!-- # # Calculate L1 models profit and append to profits list -->
<!-- # for (model in model_names) { -->
<!-- #   total_calls <- cms[[model]]$table[2, 1] + cms[[model]]$table[2, 2] -->
<!-- #   success_percentage <- cms[[model]]$table[2, 2] / total_calls -->
<!-- #   total_revenue <- total_calls * success_percentage * revenue_per_successful_call -->
<!-- #   total_cost <- total_calls * cost_per_call -->
<!-- #   profit <- total_revenue - total_cost -->
<!-- #   profits[[model]] <- profit -->
<!-- # } -->
<!-- #  -->
<!-- # # Function to calculate stacked models profit -->
<!-- # calculate_profit <- function(cm, revenue_per_successful_call, cost_per_call, margin = 0.3) { -->
<!-- #     total_calls <- cm$table[2, 1] + cm$table[2, 2] -->
<!-- #     success_percentage <- cm$table[2, 2] / total_calls -->
<!-- #     total_revenue <- total_calls * success_percentage * revenue_per_successful_call -->
<!-- #     total_cost <- total_calls * cost_per_call -->
<!-- #     profit <- total_revenue - total_cost -->
<!-- #     return(round(profit / margin, digits = 0)) -->
<!-- # } -->
<!-- # profits[['cm_sm']] <- calculate_profit(cm_sm, revenue_per_successful_call, cost_per_call) -->
<!-- # profits[['cm_sm_cost']] <- calculate_profit(cm_sm_cost, revenue_per_successful_call, cost_per_call) -->
<!-- #  -->
<!-- # # Merge profit to accuracy table -->
<!-- # accuracy_table$Profit <- unlist(profits) -->
<!-- #  -->
<!-- # # Sort by Profit -->
<!-- # accuracy_table <- accuracy_table[order(accuracy_table$Profit, decreasing = TRUE), ] -->
<!-- # rownames(accuracy_table) <- NULL -->
<!-- #  -->
<!-- # # Make table into kable -->
<!-- # kable(accuracy_table, caption = 'Accuracy Table by Model') |> -->
<!-- #   footnote(general = 'All metrics are calculated using the test set.', -->
<!-- #            general_title = 'Please Note: ', -->
<!-- #            footnote_as_chunk = T) -->
<!-- ``` -->