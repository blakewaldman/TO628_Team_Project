# Random Forest (RF)

## R Packages and Setup {-}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(randomForest)
```


## Introduction

Below paragraph is copied from the assignment, we are planning on updating this as we formulate our narrative for the final deliverable:

Random Forest (RF) is an ensemble learning method, combining multiple decision trees to reduce overfitting and handle complex relationships. RF constructs numerous decision trees during training, with each tree casting a vote on predictions. The final prediction is derived from the majority vote of all individual trees, minimizing overfitting and enhancing accuracy (Majority Vote System). Although an advanced form of DT, RF can be computationally expensive due to longer training times compared to simpler models. We will build two RF models: a basic and fine-tuned version.


## Load Data

```{r}
# Load data
train_rf <- read.csv('Train Test Set/train_rf.csv')
test_rf <- read.csv('Train Test Set/test_rf.csv')

# Display statistics
str(train_rf)
summary(train_rf)
head(train_rf)
```


## Model RF

### Simple RF

We will begin by constructing a basic RF model with 500 trees, followed by fine-tuning our RF model through grid search.

```{r model_rf, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
# Set seed for reproducibility
set.seed(12345)

# Build a model
model_rf <- randomForest(as.factor(obesity_leveloverweight) ~ .,
                         data = train_rf,
                         ntree = 500)
```

```{r model_rf_summary}
# Display summary
summary(model_rf)
randomForest::varImpPlot(model_rf)
```

According to the variable importance plot, we observe a trend comparable to that of a DT model with an implemented cost matrix, indicating that RF models may be capturing the underlying dynamics between predictors effectively. The top-ranked features in this analysis are BMI, weight-to-height ratio, and water intake per weight.

### Tuned RF

Next, we will perform tuning for our model by testing different values of `mtry`, which specifies the number of candidate features to consider at each node during tree construction. We will also execute 10-fold cross validation.

```{r model_rf_tuned, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
# Set seed for reproducibility
set.seed(12345)

# Set up control parameters
ctrl <- trainControl(method = 'cv', 
                     number = 10,
                     verboseIter = TRUE, 
                     savePredictions = TRUE)

# Define the parameter grid
grid <- expand.grid(mtry = c(2, 4, 6, 8))

# Tune the model
rf_tuned <- train(as.factor(obesity_leveloverweight) ~ ., 
                  data = train_rf, 
                  method = 'rf', 
                  trControl = ctrl, 
                  tuneGrid = grid)

# Retrieve best model
model_rf_tuned <- rf_tuned$finalModel
```

```{r model_rf_tuned_summary}
# Display summary
print(rf_tuned)
summary(model_rf_tuned)
randomForest::varImpPlot(model_rf_tuned)
```

The final selected value for our model was `mtry = 2`, as it presented the highest Kappa score without compromising overall accuracy.

## Evaluate RF

### Simple RF

```{r evaluate_rf}
# Make a prediction
prediction_rf <- predict(model_rf, test_rf)
prediction_rf_prob <- predict(model_rf, test_rf, type = 'prob')

summary(prediction_rf)

# Perform confusion matrix
cm_rf <- confusionMatrix(as.factor(prediction_rf),
                         as.factor(test_rf$obesity_leveloverweight),
                         positive = '1')
cm_rf
```

Based on the confusion matrix, we observe that the model achieves an accuracy of ``r round(cm_rf$overall[1], 4)``, sensitivity of ``r round(cm_rf$byClass[1], 4)``, and a Kappa coefficient of ``r round(cm_rf$overall[2], 4)``. We will assess these results towards the conclusion of the assignment when we have completed stacked models.

```{r}
pred <- ROCR::prediction(prediction_rf_prob[,2], test_rf$obesity_leveloverweight)
perf <- ROCR::performance(pred, measure = "tpr", x.measure = "fpr")
auc <- ROCR::performance(pred, measure="auc")


auc <- round(auc@y.values[[1]],3)
roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="GLM")

ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2, fill = "blue") +
    geom_line(aes(y=tpr), col = "blue") +
    geom_abline(intercept = 0, slope = 1, lty = "dashed") +
    labs(title = paste0("Simple Random Forest ROC Curve w/ AUC=", auc)) +
    theme_bw()
```

```{r write_prediction_rf, echo=FALSE}
# Write to csv
write.csv(prediction_rf,
          file = 'Predictions/prediction_rf.csv',
          row.names = FALSE)

# Extract confusion matrix
write.csv(cm_rf$table,
          file = 'Confusion Matrix/cm_rf.csv',
          row.names = FALSE)
```

### Tuned RF

```{r evaluate_rf_tuned}
# Make a prediction
prediction_rf_tuned <- predict(model_rf_tuned, test_rf)

prediction_rf_tuned_prob <- predict(model_rf_tuned, test_rf, type = 'prob')

summary(prediction_rf_tuned)

# Perform confusion matrix
cm_rf_tuned <- confusionMatrix(as.factor(prediction_rf_tuned),
                               as.factor(test_rf$obesity_leveloverweight),
                               positive = '1')
cm_rf_tuned
```

Based on the confusion matrix, we observe that the model achieves an accuracy of ``r round(cm_rf_tuned$overall[1], 4)``, sensitivity of ``r round(cm_rf_tuned$byClass[1], 4)``, and a Kappa coefficient of ``r round(cm_rf_tuned$overall[2], 4)``. We will assess these results towards the conclusion of the assignment when we have completed stacked models.

```{r}
pred <- ROCR::prediction(prediction_rf_tuned_prob[,2], test_rf$obesity_leveloverweight)
perf <- ROCR::performance(pred, measure = "tpr", x.measure = "fpr")
auc <- ROCR::performance(pred, measure="auc")


auc <- round(auc@y.values[[1]],3)
roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="GLM")

ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2, fill = "blue") +
    geom_line(aes(y=tpr), col = "blue") +
    geom_abline(intercept = 0, slope = 1, lty = "dashed") +
    labs(title = paste0("Tuned Random Forest ROC Curve w/ AUC=", auc)) +
    theme_bw()
```

```{r write_prediction_rf_tuned, echo=FALSE}
# Write to csv
write.csv(prediction_rf_tuned,
          file = 'Predictions/prediction_rf_tuned.csv',
          row.names = FALSE)

# Extract confusion matrix
write.csv(cm_rf_tuned$table,
          file = 'Confusion Matrix/cm_rf_tuned.csv',
          row.names = FALSE)
```

