# Random Forest (RF)

<!-- ## R Packages and Setup {-} -->

```{r load_libraries_07, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(randomForest)
```


## Introduction

Similar to decision trees, random forest is an ensemble learning method built upon decision trees to reduce overfitting and handle more complex relationships. Random forest constructs numerous decision trees during training, with each tree casting a vote on predictions. The final prediction is determined by the majority vote of all individual trees (Majority Vote System), minimizing overfitting and enhancing accuracy.

In this module, we will develop two versions of the model: a basic RF consisting of 500 trees, and another fine-tuned version.


## Load Data

```{r load_data_07}
# Load data
train_rf <- read.csv('Train Test Set/train_rf.csv')
test_rf <- read.csv('Train Test Set/test_rf.csv')

# Display statistics
str(train_rf)
summary(train_rf)
head(train_rf)
```


## Model RF

### Simple RF

We will begin by constructing a basic RF model with 500 trees, followed by fine-tuning our RF model through cross validation grid search.

```{r model_rf_07, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
# Set seed for reproducibility
set.seed(12345)

# Build a model
model_rf <- randomForest(
  as.factor(obesity_leveloverweight) ~ .,
  data = train_rf,
  ntree = 500
  )
```

```{r model_rf_summary_07}
# Display summary
summary(model_rf)
randomForest::varImpPlot(model_rf)
```

According to the variable importance plot, we observe a trend comparable to that of a DT model with an implemented cost matrix, indicating that RF models may be capturing the underlying dynamics between predictors effectively. The top-ranked features in this analysis are `age`, `fam_history_overweight`, and `food_between_meals_sometimes.`

### Tuned RF

Next, we will tune our model by examining various values of `mtry`, which determines the number of candidate features to consider at each node during tree construction. We will execute 10-fold cross validation.

```{r model_rf_tuned_07, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
# Set seed for reproducibility
set.seed(12345)

# Set up control parameters
ctrl <- trainControl(
  method = 'cv',
  number = 10,
  # verboseIter = TRUE,
  # savePredictions = TRUE
  )

# Define the parameter grid
grid <- expand.grid(
  mtry = c(2, 4, 6, 8)
)

# Tune the model
rf_tuned <- train(
  as.factor(obesity_leveloverweight) ~ .,
  data = train_rf,
  method = 'rf',
  trControl = ctrl,
  tuneGrid = grid
  )

# Retrieve best model
model_rf_tuned <- rf_tuned$finalModel
```

```{r model_rf_tuned_summary_07}
# Display summary
print(rf_tuned)
summary(model_rf_tuned)
randomForest::varImpPlot(model_rf_tuned)
```

The final selected value for our model was `r rf_tuned$finalModel$mtry`, as it presented the highest Kappa score without compromising overall accuracy.

Here we see almost identical variable importance plot compared to the none tuned version. in this instance we see `water_daily`.

## Evaluate RF

### Simple RF

```{r evaluate_rf_07}
# Make a prediction
prediction_rf <- predict(model_rf, test_rf)
prediction_rf_prob <- predict(model_rf, test_rf, type = 'prob')

summary(prediction_rf)

# Perform confusion matrix
cm_rf <- confusionMatrix(
  as.factor(prediction_rf),
  as.factor(test_rf$obesity_leveloverweight),
  positive = '1'
  )
cm_rf
```

Based on the confusion matrix, we observe that the model achieves an accuracy of ``r round(cm_rf$overall[1], 4)``, sensitivity of ``r round(cm_rf$byClass[1], 4)``, and a Kappa coefficient of ``r round(cm_rf$overall[2], 4)``. We will assess these results towards the conclusion of the assignment when we have completed stacked models.

```{r plot_auc_rf_07}
# Plot AUC
pred <- ROCR::prediction(
  prediction_rf_prob[,2],
  test_rf$obesity_leveloverweight
  )
perf <- ROCR::performance(pred, measure = "tpr", x.measure = "fpr")
auc <- ROCR::performance(pred, measure="auc")

auc <- round(auc@y.values[[1]],3)
roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="GLM")

ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2, fill = "blue") +
    geom_line(aes(y=tpr), col = "blue") +
    geom_abline(intercept = 0, slope = 1, lty = "dashed") +
    labs(title = paste0("Simple Random Forest ROC Curve w/ AUC=", auc)) +
    theme_bw()
```

```{r write_prediction_rf_07, echo=FALSE}
# Write to csv
write.csv(
  prediction_rf,
  file = 'Predictions/prediction_rf.csv',
  row.names = FALSE
  )

# Extract confusion matrix
write.csv(
  cm_rf$table,
  file = 'Confusion Matrix/cm_rf.csv',
  row.names = FALSE
  )
```

### Tuned RF

```{r evaluate_rf_tuned_07}
# Make a prediction
prediction_rf_tuned <- predict(model_rf_tuned, test_rf)

prediction_rf_tuned_prob <- predict(model_rf_tuned, test_rf, type = 'prob')

summary(prediction_rf_tuned)

# Perform confusion matrix
cm_rf_tuned <- confusionMatrix(
  as.factor(prediction_rf_tuned),
  as.factor(test_rf$obesity_leveloverweight),
  positive = '1'
  )
cm_rf_tuned
```

Based on the confusion matrix, we observe that the model achieves an accuracy of ``r round(cm_rf_tuned$overall[1], 4)``, sensitivity of ``r round(cm_rf_tuned$byClass[1], 4)``, and a Kappa coefficient of ``r round(cm_rf_tuned$overall[2], 4)``. We will assess these results towards the conclusion of the assignment when we have completed stacked models.

```{r plot_auc_rf_tuned_07}
# Plot AUC
pred <- ROCR::prediction(
  prediction_rf_tuned_prob[,2],
  test_rf$obesity_leveloverweight
  )
perf <- ROCR::performance(pred, measure = "tpr", x.measure = "fpr")
auc <- ROCR::performance(pred, measure="auc")

auc <- round(auc@y.values[[1]],3)
roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="GLM")

ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2, fill = "blue") +
    geom_line(aes(y=tpr), col = "blue") +
    geom_abline(intercept = 0, slope = 1, lty = "dashed") +
    labs(title = paste0("Tuned Random Forest ROC Curve w/ AUC=", auc)) +
    theme_bw()
```

```{r write_prediction_rf_tuned_07, echo=FALSE}
# Write to csv
write.csv(
  prediction_rf_tuned,
  file = 'Predictions/prediction_rf_tuned.csv',
  row.names = FALSE
  )

# Extract confusion matrix
write.csv(
  cm_rf_tuned$table,
  file = 'Confusion Matrix/cm_rf_tuned.csv',
  row.names = FALSE
  )
```